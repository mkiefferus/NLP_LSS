{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxg_49Pykv4f"
      },
      "source": [
        "# Build a Causal Language Model Using PyTorch\n",
        "In notebook07 we learned how to build a causal language model (GPT) from scratch. In this homework, we will build a Causal Language model with PyTorch, which also supports back-propagation compared to the notebook implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P7zU94KM3shV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9wkGm3c4exk"
      },
      "source": [
        "We are going to train our GPT on this corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVXpPsyg35Kn",
        "outputId": "1c316abd-a3ee-4537-cb0e-ead9c842e614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-07 14:34:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-07 14:34:23 (20.3 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh4FqwhvGLFb"
      },
      "source": [
        "For simplicity, character-level tokenization is applied here instead of learning a new BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bhh1K4sD4lxG"
      },
      "outputs": [],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL_gJtvIHcXK"
      },
      "source": [
        "### TODO1: Single-head Attention Implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "K6AwnR4N0Kdv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dagStHrbGci4"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # register_buffer simply adds self.tril as a non-paramter buffer in this nn.Module\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: fill in the forward pass for single-head attention\n",
        "        # Hint 1: use self.tril as the attention mask\n",
        "        # Hint 2: use torch.tensor.masked_fill to mask out the attention scores\n",
        "        # Hint 3: See the MultiHeadAttention for how the heads are concatenated\n",
        "        #print('1')\n",
        "        attention = torch.matmul(self.query(x), self.key(x).transpose(-1, -2)) / np.sqrt(self.key.weight[0].shape[-1])\n",
        "        #(self.query @ self.key.T) / self.query.shape[-1]\n",
        "        #print('2')\n",
        "        attention = attention.masked_fill(self.tril == 0, float('-inf'))\n",
        "        #print('3')\n",
        "        weights = F.softmax(attention, dim=-1)\n",
        "        #print('4')\n",
        "        weigths = self.dropout(weights)\n",
        "\n",
        "        out = torch.matmul(weights, self.value(x))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9PDdtZCIvAB"
      },
      "source": [
        "### Multi-head attention and feed forward layer implementations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "VwoaebhhIoQw"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBSMLdl5I-GJ"
      },
      "source": [
        "### TODO2: Transformer Block Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ggI9pHGSI-aS"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: fill in the forward pass\n",
        "        # Hint: don't forget the residual connections\n",
        "        \n",
        "        attention = self.sa(x)\n",
        "\n",
        "        # Idea of residual layer: tackle vanishing gradient problem during training\n",
        "        # Mechanism:  adding input of layer to output of same layer, then passing the sum\n",
        "        #             to activation function. layer does not learn direct mapping, model \n",
        "        #             learns residual function which can be added to the input to obtain\n",
        "        #             desired output. improves stability and convergence of training\n",
        "        x = x + attention\n",
        "\n",
        "        x = self.ln1(x)\n",
        "\n",
        "        ffwd = self.ffwd(x)\n",
        "        x = x + ffwd\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiSY9oQLJnzE"
      },
      "source": [
        "### Language Model Implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LNMoB0I9JWnt"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M99dsYCOMUfx",
        "outputId": "5224cb1f-b254-4cec-a27e-c83024f520ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.3197, val loss 4.3146\n",
            "step 100: train loss 2.6298, val loss 2.6392\n",
            "step 200: train loss 2.4888, val loss 2.4850\n",
            "step 300: train loss 2.3891, val loss 2.3959\n",
            "step 400: train loss 2.3477, val loss 2.3714\n",
            "step 500: train loss 2.2766, val loss 2.2868\n",
            "step 600: train loss 2.2253, val loss 2.2538\n",
            "step 700: train loss 2.2045, val loss 2.2219\n",
            "step 800: train loss 2.1493, val loss 2.1729\n",
            "step 900: train loss 2.1066, val loss 2.1423\n",
            "step 999: train loss 2.0820, val loss 2.1216\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DUKINGININGE\n",
            "AND GERLUA:\n",
            "I in podivony's, thid bist efford, pare you oulis macksturvy nome\n",
            "the seare, sheem the eall muteatt fo'd hen hast Cinhfly inst cinstion.\n",
            "\n",
            "KICERAP:\n",
            "SelauR worly, now fasere.\n",
            "I, wher arly ye the of iter.\n",
            "\n",
            "Mat ELARTiR:\n",
            "Tilly the brone, us vhard this, dext;\n",
            "Theerusp areo, my lives I'd your\n",
            "therid the thee cwellseormy id Duld of by, worderd!\n",
            "\n",
            "Herof then mellind ut suour sty, dimter\n",
            "thous has told be and he copll wiss.\n",
            "\n",
            "TERITIf:\n",
            "His madif canith thos an then peett\n",
            "Hising: the fourester douseiosh, hoy the ore hih his romen muscis an artervy;\n",
            "Ande ather noth not I thy what my frear,\n",
            "I arent theme he sallser so you\n",
            "tre driancese, faur Vedelseec:\n",
            "Hamre Iad your for I sartsce matlad fites\n",
            "Off neteaths riatiLeghfeariner:\n",
            "Onoune fe lordy en; you-hate of''d,\n",
            "you thee, his me thath that what ear brenssed\n",
            "Nrovirse nand.\n",
            " Ahe, Semel, I sand, ower corling sark, of ther thes an'se a ase dinge.\n",
            "\n",
            "ADWEO:\n",
            "Lyse hostrethether shand sam, in gesfornorble;\n",
            "Thur, yre: lis greace, wI riell us suece, herly wich parth ands,\n",
            "Thall me conerto kepe the faright I rarlye,\n",
            "Deceret nrovody, no I my hath er:\n",
            "Wor ancake, age if, he you y heurk wnote;\n",
            "Is to water it, for nouts. \n",
            "Bemomere him, your hereede.\n",
            " Aall you ce lint glaner the. the sing.\n",
            "\n",
            "EGOLETO:\n",
            "No,, now, is sery Grepase speavesesess sayerse tath I not farterth,\n",
            "Sar ale have Hone capnd?\n",
            "O, you fe, on nothe tare imjedemes: ford!\n",
            "\n",
            "PLAPARWAP:\n",
            "Bes wit dond Gind, with lic; mit nord; thee dead,'d.\n",
            "\n",
            "ELLA:\n",
            "Shone tisger te hanee.\n",
            "\n",
            "Parrecul usen dardrengesthid bettry,\n",
            "Tthationge for elfe bovenge, and waste I had ald the it speand, and you droblesor no puar ben that my to wonglly:\n",
            "I laswess ared donty, he ple prinisnsts,\n",
            "Ainly cemp gome we is a othoul\n",
            "off fresesher geeche weld nos peadl: willang seck bed prialcestes.\n",
            "Ancand thee uther.\n",
            "\n",
            "CUSNIO:\n",
            "Ans my one an ur sheaul thist heargss,\n",
            "Home ishas youll, henmim thy:\n",
            "Agras noth, sorkeshall\n",
            "'t on:\n",
            "And:\n",
            "Of sancter, bubs you se andionben,\n",
            "In Iome arlaichten, hime dands, gook.\n",
            "\n",
            "Heres! Eve, ag\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "model = LanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 32), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(context)"
      ],
      "metadata": {
        "id": "a51JPaTwsR1R",
        "outputId": "67a4381a-aeec-4d3f-8261-669e379b89d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "clS9sEanyeAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}