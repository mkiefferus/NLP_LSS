{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d21e1ead",
      "metadata": {
        "id": "d21e1ead"
      },
      "source": [
        "# HW04: ML and DL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680d1f0b",
      "metadata": {
        "id": "680d1f0b"
      },
      "source": [
        "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9bf38c8",
      "metadata": {
        "id": "c9bf38c8"
      },
      "source": [
        "## Load and Pre-process Text\n",
        "We do sentiment analysis on the [Movie Review Data](https://www.cs.cornell.edu/people/pabo/movie-review-data/). If you would like to know more about the data, have a look at [the paper](https://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf) (but no need to do so)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21439804",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21439804",
        "outputId": "60a9e6c3-e3ab-467e-bf1c-f35e31a00f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 13:21:28--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  57.8MB/s    in 1.4s    \n",
            "\n",
            "2023-03-21 13:21:29 (57.8 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "--2023-03-21 13:21:40--  https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4029756 (3.8M) [application/x-gzip]\n",
            "Saving to: ‘scale_data.tar.gz’\n",
            "\n",
            "scale_data.tar.gz   100%[===================>]   3.84M  1.27MB/s    in 3.0s    \n",
            "\n",
            "2023-03-21 13:21:43 (1.27 MB/s) - ‘scale_data.tar.gz’ saved [4029756/4029756]\n",
            "\n",
            "--2023-03-21 13:21:43--  https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8853204 (8.4M) [application/x-gzip]\n",
            "Saving to: ‘scale_whole_review.tar.gz’\n",
            "\n",
            "scale_whole_review. 100%[===================>]   8.44M  2.77MB/s    in 3.0s    \n",
            "\n",
            "2023-03-21 13:21:47 (2.77 MB/s) - ‘scale_whole_review.tar.gz’ saved [8853204/8853204]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# In this tutorial, we do sentiment analysis\n",
        "# download the data\n",
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar xf aclImdb_v1.tar.gz\n",
        "\n",
        "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
        "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
        " \n",
        "!tar xf scale_data.tar.gz \n",
        "!tar xf scale_whole_review.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d685ef2e",
      "metadata": {
        "id": "d685ef2e"
      },
      "source": [
        "First, we have to load the data for which we provide the function below. Note how we also preprocess the text using gensim's simple_preprocess() function and how we already split the data into a train and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a18a238d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a18a238d",
        "outputId": "5a5e51d4-a108-415d-d504-b27a46f0dcdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: starring jodie foster liam neeson natasha richardson richard libertini nick searcy director michael apted producers renee missel and jodie foster screenplay william nicholson and mark handley based on the play idioglossia by mark handley cinematography dante spinotti music mark isham released by twentieth century fox nell jodie foster return to dramatic acting following flirtation with maverick action comedy is an entirely human movie in this lush green world of rolling hills and crystal pools technology is an unwelcome intruder civilization threatening monster both are slaves to the avaricious nell is about the importance of communication and interaction about how the events of childhood shape life and about the difficulty and rewards of reaching out to others nell foster has lived her entire life alone in the woods with an aging mother she is eventually discovered by local doctor jerome lovell liam neeson who comes to her secluded ramshackle hut on the occasion of her mother death nell panicked and hostile response to strangers forces jerome to travel to nearby charlotte to recruit the expert assistance of dr paula olsen natasha richardson after meeting the young woman paula impression is that nell should be committed jerome horrified by this possibility obtains court order to stop it the two opposing sides eventually argue the case before judge deferring his verdict for three months the judge gives the two doctors that time to observe their subject learn her language and present him with enough evidence to make an informed decision jodie foster two oscars are no fluke as her simple yet profound performance in nell illustrates she is one of only few actors capable of so fully immersing herself in character that it possible to forget the star behind the performance with jack nicholson or al pacino you watch variation of the same personality with jodie foster you see new individual nell is an almost childlike woman who speaks her own fractured form of english hides inside her house by day and takes moonlight swims in nearby lake in jerome and paula she finds substitute father and mother and together these three attempt to breach the non physical walls between them this the real meat of nell is where the film attains its depth and richness at times director michael apted perhaps best known for his documentaries the seven up series incident at oglala seems as enamored with the scenery as with his actors the north carolina terrain is breathtaking and apted along with cinematographer dante spinotti has created one of the most beautiful motion pictures of the year it wonderful background for this tale liam neeson and natasha richardson are solid but constantly in foster shadow in film as frequently introspective as nell it refreshing not to have the three main actors struggling to outdo each other the gentle unforced tone once established is maintained until the unfortunate climax sadly scent of woman disease has infected nell perhaps the writers couldn think of better way out of an admittedly difficult situation but to pander to the hollywood mentality of movie endings is unworthy way to wrap up what is otherwise finely crafted motion picture had the majority of nell not been so impressive the lackluster final act wouldn have been as disappointing despite this moderate tarnish it is difficult to deny nell intelligence and sensitivity we approach this story with the same fascination that nell faces each day seeing if only for short time how different the world and people can be it is this impression more than any other that stays with the viewer after the drama has been played out and the final credits roll the above represents the opinions of the author and not necessarily those of bellcore or any organization within bellcore \n",
            "label: 0.78\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from gensim.utils import simple_preprocess\n",
        "def load_data():\n",
        "    examples, labels = [], []\n",
        "    authors = os.listdir(\"scale_whole_review\")\n",
        "    for author in authors:\n",
        "        path = os.listdir(os.path.join(\"scale_whole_review\", author, \"txt.parag\"))\n",
        "        fn_ids = os.path.join(\"scaledata\", author, \"id.\" + author)\n",
        "        fn_ratings = os.path.join(\"scaledata\", author, \"rating.\" + author)\n",
        "        with open(fn_ids) as ids, open(fn_ratings) as ratings:\n",
        "            for idx, rating in zip(ids, ratings):\n",
        "                labels.append(float(rating.strip()))\n",
        "                filename_text = os.path.join(\"scale_whole_review\", author, \"txt.parag\", idx.strip() + \".txt\")\n",
        "                with open(filename_text, encoding='latin-1') as f:\n",
        "                    examples.append(\" \".join(simple_preprocess(f.read())))\n",
        "    return examples, labels\n",
        "                  \n",
        "X,y  = load_data()\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "print (\"text:\", X_train[0], \"\\nlabel:\", y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284033cf",
      "metadata": {
        "id": "284033cf"
      },
      "source": [
        "## Vectorize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "09aff185",
      "metadata": {
        "id": "09aff185"
      },
      "outputs": [],
      "source": [
        "# train a TF_IDF Vectorizer on X_train and vectorize X_train and X_test\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vec = TfidfVectorizer(min_df=0.01, # at min 1% of docs\n",
        "                        max_df=.5,  \n",
        "                        stop_words='english',\n",
        "                        ngram_range=(1,2))\n",
        "\n",
        "##TODO train vectorizer\n",
        "vec.fit(X_train)\n",
        "\n",
        "# labels y do not have to be transformed as they already are in the correct range\n",
        "\n",
        "##TODO transform X_train to TF-IDF values\n",
        "X_train_tfidf = vec.transform(X_train)\n",
        "##TODO transform X_test to TF-IDF values\n",
        "X_test_tfidf = vec.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "58d44dbb",
      "metadata": {
        "id": "58d44dbb"
      },
      "outputs": [],
      "source": [
        "##TODO scale both training and test data with the standard scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "\n",
        "scaler.fit(X_train_tfidf)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train_tfidf)\n",
        "X_test_scaled = scaler.transform(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9d8a57",
      "metadata": {
        "id": "ad9d8a57"
      },
      "source": [
        "## ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1e5f4520",
      "metadata": {
        "id": "1e5f4520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacf975e-4865-4272-d2cc-6ee21b6e230c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.015281720757489, r2: 0.5276787089386865\n"
          ]
        }
      ],
      "source": [
        "##TODO train an elastic net on the transformed output of the scaler\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "en = ElasticNet(alpha=0.01)\n",
        "\n",
        "##TODO train the ElasticNet\n",
        "\n",
        "en.fit(X_train_scaled, y_train)\n",
        "\n",
        "##TODO predict the testset\n",
        "\n",
        "y_pred = en.predict(X_test_scaled)\n",
        "\n",
        "from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, balanced_accuracy_score\n",
        "##TODO print mean squared error and r2 score on the test set\n",
        "\n",
        "# Both scores evaluate the performance of regression models\n",
        "# The balanced_accuracy_score is a classification metric (thus finds no application in this task)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print('MSE: {}, r2: {}'.format(mse, r2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872d1ef8",
      "metadata": {
        "id": "872d1ef8"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27e2756e",
      "metadata": {
        "id": "27e2756e"
      },
      "source": [
        "Next, we train an OLS model doing binary prediction on these movie reviews. Two get two bins, we transform the continuous ratings into two classes, where one class contains all the negative ratings (value < 0.5), the other class all the positive ratings (value > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9cbd752c",
      "metadata": {
        "id": "9cbd752c"
      },
      "outputs": [],
      "source": [
        "y_train = [1 if i >= 0.5 else 0 for i in y_train]\n",
        "y_test = [1 if i >= 0.5 else 0 for i in y_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2c2c239d",
      "metadata": {
        "id": "2c2c239d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d98ae6c3-42ed-4b35-9d32-0ad0c2644c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.7418131941030024\n"
          ]
        }
      ],
      "source": [
        "##TODO train logistic regression on X_train\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(max_iter=200)\n",
        "\n",
        "##TODO train a logistic regression\n",
        "\n",
        "logistic_regression.fit(X_train_scaled, y_train)\n",
        "\n",
        "##TODO predict the testset\n",
        "\n",
        "y_pred = logistic_regression.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "##since we have continuous output, we need to post-process our labels into two classes. We choose a threshold of 0.5 \n",
        "def map_predictions(predicted):\n",
        "    predicted = [1 if i > 0.5 else 0 for i in predicted]\n",
        "    return predicted\n",
        "\n",
        "##TODO print the accuracy of our classifier on the testset\n",
        "\n",
        "acc = balanced_accuracy_score(y_test, map_predictions(y_pred))\n",
        "print('Acc: {}'.format(acc))\n",
        "\n",
        "## TODO print the 10 most informative words of the regression (the 10 words having the highest coefficients)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning"
      ],
      "metadata": {
        "id": "3hNKx6fUGgCL"
      },
      "id": "3hNKx6fUGgCL"
    },
    {
      "cell_type": "markdown",
      "id": "d0a6bc62",
      "metadata": {
        "id": "d0a6bc62"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the AG news dataset (same as hw01)\n",
        "#Download them from here \n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "df.columns = [\"label\", \"title\", \"lead\"]\n",
        "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
        "def replace_label(x):\n",
        "\treturn label_map[x]\n",
        "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
        "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
        "df = df.sample(n=10000) # # only use 10K datapoints\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ueplJsWuS_zl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "7cc42ee5-51d0-4094-b289-7162c60d5a32"
      },
      "id": "ueplJsWuS_zl",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 13:22:11--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29470338 (28M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]  28.10M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-03-21 13:22:13 (223 MB/s) - ‘train.csv’ saved [29470338/29470338]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           label                                              title  \\\n",
              "115288     sport                      Overreaction to 'distraction'   \n",
              "119850     sport                        Void is filled with Clement   \n",
              "119139  sci/tech                          MS plugs weak XP firewall   \n",
              "76674      world  Thai PM to Address Nation as More Bombs Hit So...   \n",
              "98515      sport                                   UNC upends UConn   \n",
              "\n",
              "                                                     lead  \\\n",
              "115288  The guy to whom everything is a potential and ...   \n",
              "119850  With the supply of attractive pitching options...   \n",
              "119139  Microsoft this week quietly fixed a security w...   \n",
              "76674   Reuters - Bomb blasts rocked southern\\Thailand...   \n",
              "98515    quot;I think the game played out probably the...   \n",
              "\n",
              "                                                     text  \n",
              "115288  Overreaction to 'distraction' The guy to whom ...  \n",
              "119850  Void is filled with Clement With the supply of...  \n",
              "119139  MS plugs weak XP firewall Microsoft this week ...  \n",
              "76674   Thai PM to Address Nation as More Bombs Hit So...  \n",
              "98515   UNC upends UConn  quot;I think the game played...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8374fbe1-1872-4b94-ad81-ff04f4f3028a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>title</th>\n",
              "      <th>lead</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>115288</th>\n",
              "      <td>sport</td>\n",
              "      <td>Overreaction to 'distraction'</td>\n",
              "      <td>The guy to whom everything is a potential and ...</td>\n",
              "      <td>Overreaction to 'distraction' The guy to whom ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119850</th>\n",
              "      <td>sport</td>\n",
              "      <td>Void is filled with Clement</td>\n",
              "      <td>With the supply of attractive pitching options...</td>\n",
              "      <td>Void is filled with Clement With the supply of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119139</th>\n",
              "      <td>sci/tech</td>\n",
              "      <td>MS plugs weak XP firewall</td>\n",
              "      <td>Microsoft this week quietly fixed a security w...</td>\n",
              "      <td>MS plugs weak XP firewall Microsoft this week ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76674</th>\n",
              "      <td>world</td>\n",
              "      <td>Thai PM to Address Nation as More Bombs Hit So...</td>\n",
              "      <td>Reuters - Bomb blasts rocked southern\\Thailand...</td>\n",
              "      <td>Thai PM to Address Nation as More Bombs Hit So...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98515</th>\n",
              "      <td>sport</td>\n",
              "      <td>UNC upends UConn</td>\n",
              "      <td>quot;I think the game played out probably the...</td>\n",
              "      <td>UNC upends UConn  quot;I think the game played...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8374fbe1-1872-4b94-ad81-ff04f4f3028a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8374fbe1-1872-4b94-ad81-ff04f4f3028a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8374fbe1-1872-4b94-ad81-ff04f4f3028a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new variable \"business\" that takes value 1 if the label is business and 0 otherwise\n",
        "df['business'] = df['label'].apply(lambda x: int(x=='business'))\n",
        "y = df['business'].values\n",
        "df['business'].head()"
      ],
      "metadata": {
        "id": "df6ZVZfDTBwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e07229-de4b-4aac-ce61-601b0bdcb554"
      },
      "id": "df6ZVZfDTBwH",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "115288    0\n",
              "119850    0\n",
              "119139    0\n",
              "76674     0\n",
              "98515     0\n",
              "Name: business, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset is unbalanced\n",
        "\n",
        "print(len([1 for x in y if x == 1]))\n",
        "print(len([0 for x in y if x == 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1X8HS5YdZro",
        "outputId": "2befe1e7-f961-48f5-cdcf-9aa06847428c"
      },
      "id": "k1X8HS5YdZro",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2436\n",
            "7564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# pre-process text as you did in HW02\n",
        "def tokenize(x):\n",
        "    return [w.lemma_.lower() for w in nlp(x) if not w.is_stop and not w.is_punct and not w.is_digit]\n",
        "\n",
        "df[\"tokens\"] = df[\"text\"].apply(lambda x: tokenize(x))\n",
        "df[\"preprocessed\"] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "df[\"preprocessed_text\"] = df[\"preprocessed\"].apply(lambda x: \" \".join(x))"
      ],
      "metadata": {
        "id": "7CFbsYDMTCTt"
      },
      "id": "7CFbsYDMTCTt",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##TODO vectorize the pre-processed text using CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(df['preprocessed'])"
      ],
      "metadata": {
        "id": "Jopj3I5MUryp"
      },
      "id": "Jopj3I5MUryp",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGi_z7mVbhvu",
        "outputId": "ca6840a1-400f-4d80-816b-f045ced9ede2"
      },
      "id": "DGi_z7mVbhvu",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 21010)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6e66fc",
      "metadata": {
        "id": "9b6e66fc"
      },
      "source": [
        "Your goal here is to use features from the Vectorized text to predict whether the snippet is from a business article.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b718ae5",
      "metadata": {
        "id": "0b718ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b896da-8f3d-4b58-e2b2-813c5017b11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                [-1, 10505]     220,720,555\n",
            "              ReLU-2                [-1, 10505]               0\n",
            "           Dropout-3                [-1, 10505]               0\n",
            "            Linear-4                 [-1, 2048]      21,516,288\n",
            "              ReLU-5                 [-1, 2048]               0\n",
            "           Dropout-6                 [-1, 2048]               0\n",
            "            Linear-7                    [-1, 1]           2,049\n",
            "           Sigmoid-8                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 242,238,892\n",
            "Trainable params: 242,238,892\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.08\n",
            "Forward/backward pass size (MB): 0.29\n",
            "Params size (MB): 924.07\n",
            "Estimated Total Size (MB): 924.44\n",
            "----------------------------------------------------------------\n",
            "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
            "[0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
            "[0, 1, 0, 0, 1, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "epoch 1: train loss: 0.6592, valid loss: 0.6391, acc: 0.9042, f1: 0.7711\n",
            "[1, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 0, 1]\n",
            "[0, 0, 0, 1, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
            "[1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "[0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
            "[0, 1, 0, 0, 0, 0, 1, 0, 0, 1]\n",
            "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
            "[0, 1, 1, 0, 1, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "epoch 2: train loss: 0.6320, valid loss: 0.6365, acc: 0.9244, f1: 0.8421\n",
            "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "[0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
            "[0, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "epoch 3: train loss: 0.6315, valid loss: 0.6423, acc: 0.8982, f1: 0.7554\n",
            "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n",
            "[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 1, 1, 0, 1, 0, 1, 1, 0]\n",
            "[1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "[0, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
            "[0, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n",
            "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
            "[0, 1, 0, 0, 1, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 1, 1, 1, 0, 0, 1, 0]\n",
            "[0, 0, 1, 1, 0, 0, 0, 0, 0, 1]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "epoch 4: train loss: 0.6284, valid loss: 0.6442, acc: 0.9133, f1: 0.8266\n",
            "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 0, 1]\n",
            "[1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0, 1, 0, 1, 1, 0]\n",
            "[1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
            "[1, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "[0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
            "[0, 1, 1, 0, 1, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 1, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "epoch 5: train loss: 0.6300, valid loss: 0.6355, acc: 0.9274, f1: 0.8494\n",
            "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 0, 1]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
            "[0, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "[0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 0, 1]\n",
            "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 1, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "epoch 6: train loss: 0.6300, valid loss: 0.6424, acc: 0.9073, f1: 0.7974\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.modules.activation import Sigmoid\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchsummary import summary\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## TODO build a MLP model with at least 2 hidden layers with ReLU activation, followed by dropout and an output layer with sigmoid activation\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, dropout_p=0.2):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(in_features, in_features//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(in_features//2, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Linear(2048, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "model = MLP(X_train.shape[1]).to(device)\n",
        "\n",
        "## TODO summarize the model using torchsummary\n",
        "\n",
        "summary(model, input_size=(X_train.shape[1],))\n",
        "\n",
        "## TODO fit the model using early stopping to predict the business label\n",
        "\n",
        "tsize = math.ceil(0.1 * len(y))\n",
        "\n",
        "# stratify preserved the ratio of 0s and 1s in the dataset\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X.toarray(), np.array(y), test_size=tsize, stratify=y, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=tsize, stratify=y_train, random_state=0)\n",
        "\n",
        "class my_dataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "\n",
        "train_dataset = my_dataset(X_train, y_train)\n",
        "valid_dataset = my_dataset(X_valid, y_valid)\n",
        "test_dataset = my_dataset(X_test, y_test)\n",
        "\n",
        "# prepare dataloaders where we can load mini-batches from\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "#Train model\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Expected input datatype is float\n",
        "        # Also guarantees higher precision than int\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs.float(), labels.float().unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    model.eval()\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for j, (inputs, labels) in enumerate(valid_loader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs.float())\n",
        "            loss = criterion(outputs.float(), labels.float().unsqueeze(1))\n",
        "\n",
        "            valid_loss.append(loss.item())\n",
        "\n",
        "            predicted = [1 if d > 0.5 else 0 for d in outputs.data.squeeze()]\n",
        "\n",
        "            print(predicted[:10])\n",
        "            pred_labels.extend(predicted)\n",
        "            true_labels.extend(list(labels.detach().cpu().numpy()))\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    f1 = f1_score(true_labels, pred_labels)\n",
        "    \n",
        "    print('epoch {}: train loss: {:.4f}, valid loss: {:.4f}, acc: {:.4f}, f1: {:.4f}'.format(epoch+1, np.mean(train_loss), np.mean(valid_loss), accuracy, f1))\n",
        "\n",
        "\n",
        "\n",
        "# (hint: early stopping means if the validation score does not increase for more than \"patience\" times, training should stop and load the best model so far)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "1263d8c1",
      "metadata": {
        "id": "1263d8c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87cb126a-42b9-4fad-8490-b05487377584"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5491"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}