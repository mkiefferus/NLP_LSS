{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dbe9d25b",
      "metadata": {
        "id": "dbe9d25b"
      },
      "source": [
        "# HW05: Word Embeddings\n",
        "\n",
        "Remember that these homework work as a completion grade. **You can <span style=\"color:red\">not</span> skip one section this homework.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3a0596",
      "metadata": {
        "id": "8b3a0596"
      },
      "source": [
        "**Essay Feedback**\n",
        "\n",
        "Please provide feedback to two classmates' essays on Eduflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea14794",
      "metadata": {
        "id": "5ea14794"
      },
      "source": [
        "**Training word2vec**\n",
        "\n",
        "In this section, we train a word2vec model using gensim. We train the model on text8 (which consists of the first 90M characters of a Wikipedia dump from 2006 and is considered one of the benchmarks for evaluating language models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "95a38d6e",
      "metadata": {
        "id": "95a38d6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1bf0c3-7032-4213-dfcc-52b67b699259"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_records': 1701,\n",
              " 'record_format': 'list of str (tokens)',\n",
              " 'file_size': 33182058,\n",
              " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              " 'license': 'not found',\n",
              " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              " 'file_name': 'text8.gz',\n",
              " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              " 'parts': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "api.info(\"text8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0a49444c",
      "metadata": {
        "id": "0a49444c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afbca22-c2bb-4e51-ddd0-8a47fd487985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ],
      "source": [
        "dataset = api.load(\"text8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "61fa38b5",
      "metadata": {
        "id": "61fa38b5"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "##TODO train a word2vec model on this dataset which appear at least 10 times in the corpus\n",
        "\n",
        "model = Word2Vec(dataset, min_count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af69360",
      "metadata": {
        "id": "4af69360"
      },
      "source": [
        "**Word Similarities**\n",
        "\n",
        "gensim models provide almost all the utility you might want to wish for to perform standard word similarity tasks. They are available in the .wv (wordvectors) attribute of the model, more details could be found [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5cf99280",
      "metadata": {
        "id": "5cf99280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77dde279-7fdc-4313-d631-d8844d722222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('prince', 0.7743529081344604)\n",
            "('queen', 0.7292153239250183)\n",
            "('kings', 0.7031580209732056)\n",
            "('throne', 0.6981783509254456)\n",
            "('emperor', 0.6948561072349548)\n",
            "('regent', 0.6773486137390137)\n",
            "('sultan', 0.6719624400138855)\n",
            "('vii', 0.6605079174041748)\n",
            "('aragon', 0.6547995209693909)\n",
            "('elector', 0.6542447209358215)\n"
          ]
        }
      ],
      "source": [
        "model.wv\n",
        "\n",
        "##TODO find the closest words to king\n",
        "\n",
        "close_words = model.wv.most_similar('king')\n",
        "\n",
        "for word in close_words:\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c30c847",
      "metadata": {
        "id": "9c30c847"
      },
      "source": [
        "King is to man as woman is to X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "615b6116",
      "metadata": {
        "id": "615b6116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a0b21c-2c82-47e1-c834-2ca8de27a3ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('queen', 0.6954731941223145)\n"
          ]
        }
      ],
      "source": [
        "##TODO find the closest word for the vector \"woman\" + \"king\" - \"man\"\n",
        "\n",
        "closest_word = model.wv.most_similar(positive=['woman', 'king'], negative=['man'])[0]\n",
        "\n",
        "print(closest_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af37627",
      "metadata": {
        "id": "2af37627"
      },
      "source": [
        "**Evaluate Word Similarities** \n",
        "\n",
        "One common way to evaluate word2vec models are word analogy tasks. Let's check how good our model is on one of those. We consider the [WordSim353](http://alfonseca.org/eng/research/wordsim353.html) benchmark, the task is to determine how similar two words are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "71515b20",
      "metadata": {
        "id": "71515b20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2e8069-94a2-4be5-ac97-65169167033a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-30 13:34:53--  http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
            "Resolving alfonseca.org (alfonseca.org)... 162.215.249.67\n",
            "Connecting to alfonseca.org (alfonseca.org)|162.215.249.67|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5460 (5.3K) [application/x-gzip]\n",
            "Saving to: ‘ws353simrel.tar.gz’\n",
            "\n",
            "\rws353simrel.tar.gz    0%[                    ]       0  --.-KB/s               \rws353simrel.tar.gz  100%[===================>]   5.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-30 13:34:54 (486 MB/s) - ‘ws353simrel.tar.gz’ saved [5460/5460]\n",
            "\n",
            "[('tiger', 'cat'), ('tiger', 'tiger'), ('plane', 'car')] [7.35, 10.0, 5.77]\n"
          ]
        }
      ],
      "source": [
        "!wget http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
        "!tar xf ws353simrel.tar.gz\n",
        "\n",
        "path = \"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
        "\n",
        "def load_data(path):\n",
        "    X, y = [], []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split(\"\\t\")\n",
        "            X.append((line[0], line[1])) # each entry in x contains two words, e.g. X[0] = (tiger, cat)\n",
        "            y.append(float(line[-1])) # each entry in y is the annotation how similar two words are, e.g. Y[0] = 7.35\n",
        "    return X, y\n",
        "\n",
        "X, y = load_data(path)\n",
        "print (X[:3], y[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9c8ced33",
      "metadata": {
        "id": "9c8ced33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a730d40-973f-4960-d26a-3f81ae29662f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.61210567, 1.0, 0.4190455]\n"
          ]
        }
      ],
      "source": [
        "##TODO compute how similar the pairs in the WordSim353 are according to our model\n",
        "# if a word is not present in our model, we assign similarity 0 for the respective text pair\n",
        "\n",
        "sim_scores = []\n",
        "\n",
        "for pair in X:\n",
        "    try:\n",
        "        score = model.wv.similarity(pair[0], pair[1])\n",
        "    except KeyError:\n",
        "        score = 0.0\n",
        "    \n",
        "    sim_scores.append(score)\n",
        "\n",
        "print(sim_scores[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ebd47f93",
      "metadata": {
        "id": "ebd47f93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed797cb-3493-417e-a22b-b71b95f463f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6463631577377009\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "##TODO compute spearman's rank correlation between our prediction and the human annotations\n",
        "\n",
        "correlation, _ = spearmanr(y, sim_scores)\n",
        "\n",
        "print(correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9ec86899",
      "metadata": {
        "id": "9ec86899",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdac69c3-8e30-4dc9-cd36-8f46909c2428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-e68eac66da0e>:12: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  score = word1.similarity(word2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6628428894742415, 1.0, 0.7974166052761725]\n",
            "0.0917488312498204\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "\n",
        "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
        "\n",
        "sim_scores = []\n",
        "\n",
        "for pair in X:\n",
        "    word1 = en(pair[0])\n",
        "    word2 = en(pair[1])\n",
        "\n",
        "    score = word1.similarity(word2)\n",
        "    sim_scores.append(score)\n",
        "\n",
        "print(sim_scores[:3])\n",
        "\n",
        "##TODO compute spearman's rank correlation between these similarities and the human annotations\n",
        "# Don't worry if results are not too convincing for this experiment\n",
        "\n",
        "correlation, _ = spearmanr(y, sim_scores)\n",
        "\n",
        "print(correlation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d29de774",
      "metadata": {
        "id": "d29de774"
      },
      "source": [
        "**PyTorch Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "3927e048",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-22T21:42:21.281177Z",
          "start_time": "2022-03-22T21:42:21.208787Z"
        },
        "id": "3927e048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "17050a99-4407-40ae-adc9-860f7898d658"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          label                                          title  \\\n",
              "15599     world        12 Nepalese Reportedly Executed in Iraq   \n",
              "17422     sport                                    NL notables   \n",
              "82962     world  Kidnappers Extend Deadline on British Hostage   \n",
              "66124  sci/tech           Intel to Ship Dual-Core Xeon in 2006   \n",
              "56694     sport                   Bottom line: Boston believes   \n",
              "\n",
              "                                                    lead  \\\n",
              "15599  A video purporting to show the killings of Nep...   \n",
              "17422  The Mets' Jeff Keppinger got his first major l...   \n",
              "82962  Militants who claim to be holding Northern Iri...   \n",
              "66124  Company denies the chip has been delayed, but ...   \n",
              "56694  These Boston Red Sox aren #39;t worried about ...   \n",
              "\n",
              "                                                    text  \n",
              "15599  12 Nepalese Reportedly Executed in Iraq A vide...  \n",
              "17422  NL notables The Mets' Jeff Keppinger got his f...  \n",
              "82962  Kidnappers Extend Deadline on British Hostage ...  \n",
              "66124  Intel to Ship Dual-Core Xeon in 2006 Company d...  \n",
              "56694  Bottom line: Boston believes These Boston Red ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d14b208-2598-4bd6-80d1-87ead24c9e62\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>title</th>\n",
              "      <th>lead</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15599</th>\n",
              "      <td>world</td>\n",
              "      <td>12 Nepalese Reportedly Executed in Iraq</td>\n",
              "      <td>A video purporting to show the killings of Nep...</td>\n",
              "      <td>12 Nepalese Reportedly Executed in Iraq A vide...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17422</th>\n",
              "      <td>sport</td>\n",
              "      <td>NL notables</td>\n",
              "      <td>The Mets' Jeff Keppinger got his first major l...</td>\n",
              "      <td>NL notables The Mets' Jeff Keppinger got his f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82962</th>\n",
              "      <td>world</td>\n",
              "      <td>Kidnappers Extend Deadline on British Hostage</td>\n",
              "      <td>Militants who claim to be holding Northern Iri...</td>\n",
              "      <td>Kidnappers Extend Deadline on British Hostage ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66124</th>\n",
              "      <td>sci/tech</td>\n",
              "      <td>Intel to Ship Dual-Core Xeon in 2006</td>\n",
              "      <td>Company denies the chip has been delayed, but ...</td>\n",
              "      <td>Intel to Ship Dual-Core Xeon in 2006 Company d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56694</th>\n",
              "      <td>sport</td>\n",
              "      <td>Bottom line: Boston believes</td>\n",
              "      <td>These Boston Red Sox aren #39;t worried about ...</td>\n",
              "      <td>Bottom line: Boston believes These Boston Red ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d14b208-2598-4bd6-80d1-87ead24c9e62')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6d14b208-2598-4bd6-80d1-87ead24c9e62 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6d14b208-2598-4bd6-80d1-87ead24c9e62');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "#Import the AG news dataset (same as hw01)\n",
        "#Download them from here \n",
        "# !wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "df.columns = [\"label\", \"title\", \"lead\"]\n",
        "\n",
        "df = df.sample(n=10000) # # only use 10K datapoints\n",
        "\n",
        "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
        "def replace_label(x):\n",
        "\treturn label_map[x]\n",
        "\n",
        "y = df['label']\n",
        "\n",
        "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
        "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "a49d6b6e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-22T21:40:20.385383Z",
          "start_time": "2022-03-22T21:40:18.447956Z"
        },
        "id": "a49d6b6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830944cd-5f4c-48c1-f3ae-88f62c521204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'more', 'world', 'to', 'he', 'president', 'former', 'first', 'court', 'space', 'n', 'as', 'microsoft', 'gt', 'no', 'and', 'would', 'years', 'b', 'into', 'a', 'u', 'had', 'you', 'international', 'american', 'get', 'next', 'reported', 'what', 'oil', 'bush', 'this', 'google', 'their', 'co', 'prime', 'national', 'day', 'japan', 'india', 's', 'make', 'afp', 'officials', 'the', 'reuters', 'yesterday', 'been', 'third', 'year', 'after', 'week', 'software', 'company', 'they', 'it', 'york', 'deal', 'victory', 'quarter', 'just', 'computer', 'market', 'killed', 'may', 'expected', 'news', 'of', 'billion', 'said', 'night', 'washington', 'wednesday', 'could', 'announced', 'profit', 'high', 'when', 'end', 'will', 'home', 'by', 'says', 'plans', 'against', 'about', 't', 'last', 'lead', 'season', 'four', 'out', 'tuesday', 'be', 'three', 'baghdad', 'its', 'an', 'thursday', 'million', 'chief', 'but', 'cup', 'service', 'some', 'one', 'second', 'report', 'won', 'two', 'open', 'games', 'china', 'red', 'most', 'fullquote', 'for', 'can', 'another', 'were', 'iraq', 'which', 'european', 'who', 'web', 'ap', 'over', 'new', 'saturday', 'talks', 'major', 'inc', 'on', 'with', 'state', 'all', 'leader', 'set', 'league', 'from', 'up', 'before', 'his', 'other', 'police', 'us', 'states', 'country', 'search', 'p', 'monday', 'have', 'sunday', 'win', 'hit', 'now', 'game', 'in', 'top', 'record', 'not', 'or', 'is', 'team', 'city', 'prices', 'sales', 'time', 'at', 'are', 'five', 'united', 'lt', 'stocks', 'off', 'federal', 'back', 'today', 'online', 'quot', 'minister', 'if', 'people', 'government', 'iraqi', 'corp', 'that', 'has', 'was', 'com', 'business', 'percent', 'friday', 'month', 'group', 'down', 'security', 'under', 'internet', 'than'}\n"
          ]
        }
      ],
      "source": [
        "vocab = 200\n",
        "##TODO tokenize the text, only keep 200 most frequent words \n",
        "\n",
        "from gensim.utils import tokenize\n",
        "from collections import Counter\n",
        "\n",
        "tokenized_text = [list(tokenize(t, lowercase = True)) for t in df['text']]\n",
        "\n",
        "\n",
        "whole_corpus = []\n",
        "\n",
        "for t in tokenized_text:\n",
        "    whole_corpus.extend(t)\n",
        "\n",
        "\n",
        "common_words = set([t[0] for t in Counter(whole_corpus).most_common(200)])\n",
        "\n",
        "print(common_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "c4c0f840",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-22T21:40:23.322875Z",
          "start_time": "2022-03-22T21:40:23.311923Z"
        },
        "id": "c4c0f840",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3925a8a8-a343-480e-f890-60af73029b9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3054, 4162, 469, 4976, 1534, 294, 5894, 5741, 8172, 272, 8172, 5671, 1474, 9534, 5184, 5612, 3054, 4487, 1323, 3181, 6073, 4237, 2466, 1323, 3181, 4736, 3715, 227, 2464, 3181, 1736, 612, 1534, 294, 7741, 5894, 5741, 469, 7903, 1534, 8172]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "length = 100\n",
        "#TODO create a one_hot representation for each word and truncate/pad the sequences such that they are all of the same length (here we use 100)\n",
        "\n",
        "# !pip install Keras-Preprocessing\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_one_hot = [one_hot(opinion, n=10000) for opinion in df['text']]\n",
        "print(X_one_hot[0][:50])\n",
        "\n",
        "X_one_hot_padded = pad_sequences(X_one_hot, padding='post', maxlen=100, truncating='post')\n",
        "X_one_hot_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "c3d193dd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-22T21:40:28.364553Z",
          "start_time": "2022-03-22T21:40:28.354695Z"
        },
        "id": "c3d193dd"
      },
      "outputs": [],
      "source": [
        "##TODO create your torch embedding like we did in notebook 5! (hint: predicting labels: world, sport, business, and sci/tech)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self, num_words=100):\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_words + 1, 96)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(96 * 13924, 64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x, _ = torch.max(x, dim=-1)\n",
        "        return x\n",
        "\n",
        "class GenericDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tsize = math.ceil(0.1 * len(y))\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_one_hot_padded, np.array(y), test_size=tsize, stratify=y, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=tsize, stratify=y_train, random_state=0)\n",
        "\n",
        "train_dataset = GenericDataset(X_train, y_train)\n",
        "valid_dataset = GenericDataset(X_valid, y_valid)\n",
        "test_dataset = GenericDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=16, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False, drop_last=True)\n",
        "\n",
        "model = EmbeddingNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "2cac3bd2",
      "metadata": {
        "id": "2cac3bd2"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "f1_scores = []\n",
        "best_model = None\n",
        "train_stop = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Expected input datatype is float\n",
        "        # Also guarantees higher precision than int\n",
        "        outputs = model(inputs.int())\n",
        "        loss = criterion(outputs.int(), labels.int().unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for j, (inputs, labels) in enumerate(valid_loader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs.int())\n",
        "            loss = criterion(outputs.int(), labels.int().unsqueeze(1))\n",
        "\n",
        "            valid_loss.append(loss.item())\n",
        "\n",
        "            predicted = [1 if d > 0.5 else 0 for d in outputs.data.squeeze()]\n",
        "\n",
        "            pred_labels.extend(predicted)\n",
        "            true_labels.extend(list(labels.detach().cpu().numpy()))\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    f1 = f1_score(true_labels, pred_labels)\n",
        "\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    if f1 >= max(f1_scores):\n",
        "        best_model = model.state_dict()\n",
        "    \n",
        "    print('epoch {}: train loss: {:.4f}, valid loss: {:.4f}, acc: {:.4f}, f1: {:.4f}'.format(epoch+1, np.mean(train_loss), np.mean(valid_loss), accuracy, f1))\n",
        "\n",
        "    if len(f1_scores) - f1_scores.index(max(f1_scores)) == train_stop:\n",
        "        break\n",
        "\n",
        "torch.save(best_model, 'best_model.pt')\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "pred_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for j, (inputs, labels) in enumerate(test_loader):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs.int())\n",
        "\n",
        "        predicted = [1 if d > 0.5 else 0 for d in outputs.data.squeeze()]\n",
        "\n",
        "        pred_labels.extend(predicted)\n",
        "        true_labels.extend(list(labels.detach().cpu().numpy()))\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "\n",
        "print()\n",
        "print('Best model reaches accuracy of {:.2f}%'.format(accuracy*100))"
      ],
      "metadata": {
        "id": "VgTH_kpEMsTw",
        "outputId": "0fe956fe-ba44-4945-ce20-3b4f60054865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        }
      },
      "id": "VgTH_kpEMsTw",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5889, 7719,  870,  ...,    0,    0,    0],\n",
            "        [1296, 8160, 2478,  ...,    0,    0,    0],\n",
            "        [ 199,  699, 8831,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [9184, 3304, 6987,  ...,    0,    0,    0],\n",
            "        [8430, 9268, 2582,  ...,    0,    0,    0],\n",
            "        [7719, 3138, 6555,  ...,    0,    0,    0]], dtype=torch.int32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-97aa209baec5>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Expected input datatype is float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Also guarantees higher precision than int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-bd8ddebf64bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJie393INAGF"
      },
      "id": "fJie393INAGF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}